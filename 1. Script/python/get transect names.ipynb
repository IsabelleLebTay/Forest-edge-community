{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# processed_path = \"C:/Users/ilebe/Documents/!Masters!/Chapter 3/Data synthesis/0. Data/Processed\"\n",
    "processed_path = r\"C:\\Users\\ilebe\\Documents\\!Masters!\\Chapter 3\\Data synthesis\\0. Data\\Processed\"\n",
    "raw_path = \"C:/Users/ilebe/Documents/!Masters!/Chapter 3/Data synthesis/0. Data/Raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sites_for_transects(root_dir):\n",
    "    transects_sites = {}  # Dictionary to hold transect: [sites] pairs\n",
    "\n",
    "    # Iterate through each transect in the root directory\n",
    "    for transect in next(os.walk(root_dir))[1]:  # Gets transect directories\n",
    "        transect_path = os.path.join(root_dir, transect)\n",
    "        sites = next(os.walk(transect_path))[1]  # Gets site directories under each transect\n",
    "\n",
    "        transects_sites[transect] = sites  # Store the list of sites for this transect\n",
    "\n",
    "    return transects_sites\n",
    "\n",
    "# Specify the root directory here\n",
    "root_dir = \"X:/BU/ARU/EDGEHARV/2022/V1\"\n",
    "transects_sites = list_sites_for_transects(root_dir)\n",
    "\n",
    "print(transects_sites)\n",
    "# Print the dictionary to see the transects and their sites\n",
    "# for transect, sites in transects_sites.items():\n",
    "#     print(f\"Transect: {transect}, Sites: {sites}\")\n",
    "\n",
    "with open(os.path.join(processed_path, \"2022\", \"2022 transects.json\"), 'w') as file:\n",
    "    json.dump(transects_sites, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_edge_path = r\"X:\\BU\\ARU\\EDGEHARV\\2023\\V1\\H-E0-M\"\n",
    "sites_2023 = next(os.walk(mono_edge_path))[1]\n",
    "with open(os.path.join(processed_path, '2023', 'location names.csv'), 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerows([[site] for site in sites_2023])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_2021_list = []\n",
    "for transect, sites in transects_sites.items():\n",
    "    for loc in sites:\n",
    "        sites_2021_list.append(loc)\n",
    "\n",
    "sites_2021_list\n",
    "\n",
    "with open(os.path.join(processed_path, '2022', 'location names.csv'), 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerows([[site] for site in sites_2021_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe of site and transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_sites_for_transects_to_df(root_dir):\n",
    "    data = []  # List to hold all transect-site pairs\n",
    "\n",
    "    # Iterate through each transect in the root directory\n",
    "    for transect in next(os.walk(root_dir))[1]:\n",
    "        transect_path = os.path.join(root_dir, transect)\n",
    "        sites = next(os.walk(transect_path))[1]  # Gets site directories under each transect\n",
    "        \n",
    "        # For each site, create a dictionary and append it to the data list\n",
    "        for site in sites:\n",
    "            data.append({'location': site, 'transect': transect})\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Specify the root directory here\n",
    "root_dir = \"X:/BU/ARU/EDGEHARV/2021/V1\"\n",
    "transects_sites_df = list_sites_for_transects_to_df(root_dir)\n",
    "transects_sites_df.to_csv(os.path.join(processed_path, '2021', 'location transects.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a dataframe of the sites, years, and coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_2021 = pd.read_csv(os.path.join(processed_path, '2021', 'location transects.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_2021 = pd.read_csv(os.path.join(processed_path, '2021', 'location transects.csv'))\n",
    "sites_2021.insert(2, \"sampleyear\", 2021)\n",
    "\n",
    "sites_2022 = pd.read_csv(os.path.join(processed_path, '2022', 'location transects.csv'))\n",
    "sites_2022.insert(2, \"sampleyear\", 2022)\n",
    "\n",
    "sites_2023 = pd.read_csv(os.path.join(processed_path, '2023', 'location names.csv'), header=None, names = ['location'])\n",
    "sites_2023.insert(1, \"transect\", \"Mono\")\n",
    "sites_2023.insert(2, \"sampleyear\", 2023)\n",
    "\n",
    "harvedge = pd.concat([sites_2021, sites_2022, sites_2023])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BU = pd.read_csv(os.path.join(raw_path, \"BU_locations_20240212.csv\"), usecols=['location', 'longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvedge_coords = pd.merge(left=harvedge, right=BU, how='left', on='location')\n",
    "# harvedge_coords.isna().any()\n",
    "harvedge_coords[harvedge_coords.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvedge_coords.to_csv(os.path.join(processed_path, \"locations with coordinates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated manually missing coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvedge = pd.read_csv(os.path.join(processed_path, \"locations with coordinates.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixes to keep\n",
    "prefixes = ('H-N15', 'H-E0', 'H-P15')\n",
    "\n",
    "# Filter rows where 'location' starts with any of the specified prefixes\n",
    "filtered_df = harvedge[harvedge['location'].str.startswith(prefixes)]\n",
    "filtered_df[filtered_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One E0 site without cooordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(os.path.join(processed_path, \"edge interior open and coordinates.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
